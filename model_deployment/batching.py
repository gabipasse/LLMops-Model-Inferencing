import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast
from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel
from torch import device, cuda, Tensor
from datasets import load_dataset
from typing import Generator


def generate_text(
    prompt: str,
    tokenizer: GPT2TokenizerFast,
    model: GPT2LMHeadModel,
    avaible_device: device,
) -> str:
    """Generate prediction based on the input.

    Parameters
    ----------
    prompt: str
        Input to be inferenced from.
    tokenizer: GPT2TokenizerFast
        Tokenizer which will be used to tokenize the string input into tokenized indexed values.
    model: GPT2LMHeadModel
        Model which will be used for inference.
    avaible_device: device
        Device (CPU or GPU) on which the model's predictions and tokenization will be performed.

    Return
    ------
    str
        Response generated by the model.
    """
    inputs_tokenized = tokenizer.encode(prompt, return_tensors="pt").to(avaible_device)
    with torch.no_grad():
        outputs = model.generate(inputs_tokenized, max_length=64)
    outputs_converted = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # This indexing is necessary because gpt2 is an autoregressive model
    return outputs_converted[: outputs_converted.find(".") + 1]


def batch_generate_texts(
    prompts: list[str],
    tokenizer: GPT2TokenizerFast,
    model: GPT2LMHeadModel,
    avaible_device: device,
) -> list[str]:
    """Generate predictions from inputs, treated as batches.

    Parameters
    ----------
    prompts: list[str]
        List of inputs to be treated as batches and inferenced from.
    tokenizer: GPT2TokenizerFast
        Tokenizer which will be used to tokenize the string input into tokenized indexed values.
    model: GPT2LMHeadModel
        Model which will be used for inference.
    avaible_device: device
        Device (CPU or GPU) on which the model's predictions and tokenization will be performed.

    Return
    ------
    list[str]
        Responses generated by the model.
    """

    # Encode method is not used because here the tokenizer is dealing with a batch of inputs
    inputs_tokenized = tokenizer(prompts, return_tensors="pt", padding=True)[
        "input_ids"
    ].to(avaible_device)

    with torch.no_grad():
        outputs = model.generate(
            inputs_tokenized, max_length=64, pad_token_id=tokenizer.eos_token_id
        )

    outputs_converted = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    return outputs_converted


# Dynamic Batching
def batch_generate(
    tokens: list[Tensor], tokenizer: GPT2TokenizerFast, model: GPT2LMHeadModel
) -> list[str]:
    """Generate predictions from tokenized inputs, treated as batches.

    Parameters
    ----------
    tokens: list[Tensor]
        List of tokenized inputs to be treated as batches and inferenced from.
    tokenizer: GPT2TokenizerFast
        Tokenizer which will be used to tokenize the string input into tokenized indexed values.
    model: GPT2LMHeadModel
        Model which will be used for inference.

    Return
    ------
    list[str]
        Response generated by the model.
    """
    # Using torch.stach to transform all tensors into one, allowing batch processing by the model
    with torch.no_grad():
        outputs = model.generate(
            torch.stack(tokens), max_length=64, pad_token_id=tokenizer.eos_token_id
        )

    return tokenizer.batch_decode(outputs, skip_special_tokens=True)


def dynamic_batching(
    prompts: list[str],
    max_tokens: int,
    tokenizer: GPT2TokenizerFast,
    model: GPT2LMHeadModel,
    avaible_device: device,
) -> Generator[list[str], None, str]:
    """Applies dynamic batching technique to inference from inputs.

    Parameters
    ----------
    prompts: list[str]
        List of inputs to be treated as batches and inferenced from.
    max_tokens: int
        Maximun amount of token to be processed within the same batch.
    tokenizer: GPT2TokenizerFast
        Tokenizer which will be used to tokenize the string input into tokenized indexed values.
    model: GPT2LMHeadModel
        Model which will be used for inference.
    avaible_device: device
        Device (CPU or GPU) on which the model's predictions and tokenization will be performed.

    Yield
    ------
    Generator[list[str], None, None]
        Response generated by the model.
    """

    # Only the input_ids are moved to the avaible_device because, in this script, these are the only results used from the tokenization process
    inputs_tokenized = tokenizer(prompts, return_tensors="pt", padding=True)[
        "input_ids"
    ].to(avaible_device)
    current_batch = []
    current_batch_size = 0

    for tokenized_text in inputs_tokenized:
        if current_batch_size + len(tokenized_text) > max_tokens and current_batch:
            yield batch_generate(current_batch, tokenizer, model)
            current_batch, current_batch_size = [], 0

        current_batch.append(tokenized_text)
        current_batch_size += len(tokenized_text)

    # Process final batch
    if current_batch:
        # Check if the batch contains only one element. If so, add a dummy dimension to avoid the ValueError.
        if len(current_batch) == 1:
            # Duplicate the single element to create a 2D array
            current_batch = [current_batch[0], current_batch[0]]

        yield batch_generate(current_batch, tokenizer, model)

    return "All batches have been processed."


def main():
    # Instruc/answer dataset
    dataset = load_dataset("hakurei/open-instruct-v1", split="train")

    avaible_device = device("cuda" if cuda.is_available() else "cpu")
    # Moving the model to the GPU, but not the tokenizer, is a common practice because
    # the tokenizer only handles the conversion of text into tokens (numbers) and vice versa, and this task is not computationally intensive.
    # This model is based on the gpt2 model
    model: GPT2LMHeadModel = AutoModelForCausalLM.from_pretrained(
        "TheFuzzyScientist/diabloGPT_open-instruct"
    ).to(avaible_device)
    model.eval()
    # Left padding because the DialoGPT-medium is an autoregressive model
    tokenizer: GPT2TokenizerFast = AutoTokenizer.from_pretrained(
        "microsoft/DialoGPT-medium", padding_side="left"
    )
    # Defining padding token because, by default, gpt2 based models do not have padding tokens
    tokenizer.pad_token = tokenizer.eos_token

    print(
        generate_text("Where is Brazil located at?", tokenizer, model, avaible_device)
    )

    print(
        batch_generate_texts(
            dataset["instruction"][:100], tokenizer, model, avaible_device
        )
    )

    generator = dynamic_batching(
        dataset["instruction"][:20], 4, tokenizer, model, avaible_device
    )
    try:
        print(next(generator))
    except StopIteration as e:
        print(f"Generator returned: {e.value}")


if __name__ == "__main__":
    main()
